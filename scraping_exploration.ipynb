{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/andrewosborne/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import json\n",
    "import datetime\n",
    "import requests\n",
    "import time\n",
    "from collections import deque\n",
    "from urllib.parse import quote_plus\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempt 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScraperPipeline:\n",
    "    def __init__(self, platform, keyword, output_file=None, max_results=20):\n",
    "        self.platform = platform.lower()\n",
    "        self.keyword = keyword\n",
    "        self.max_results = max_results\n",
    "        self.output_file = output_file or f\"{self.platform}_{self.keyword.replace(' ', '_')}_{datetime.datetime.now().strftime('%m%d_%H%M%S')}.json\"\n",
    "        self.headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.4844.51 Safari/537.36',\n",
    "        }\n",
    "        \n",
    "    def get_search_url(self):\n",
    "        if self.platform == \"youtube\":\n",
    "            return f\"https://www.youtube.com/results?search_query={quote_plus(self.keyword)}\"\n",
    "        elif self.platform == \"twitter\":\n",
    "            return f\"https://twitter.com/search?q={quote_plus(self.keyword)}&src=typed_query\"\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported platform: {self.platform}\")\n",
    "    \n",
    "    def download_content(self, url):\n",
    "        if self.platform == \"twitter\":\n",
    "            # Use Selenium with headless Chrome to execute JavaScript\n",
    "            \n",
    "            # Configure Chrome options\n",
    "            chrome_options = Options()\n",
    "            chrome_options.add_argument(\"--headless\")  # Run in headless mode (no visible browser)\n",
    "            chrome_options.add_argument(\"--disable-gpu\")\n",
    "            chrome_options.add_argument(\"--no-sandbox\")\n",
    "            chrome_options.add_argument(f\"user-agent={self.headers['User-Agent']}\")\n",
    "            \n",
    "            try:\n",
    "                # Initialize the driver\n",
    "                driver = webdriver.Chrome(options=chrome_options)\n",
    "                \n",
    "                # Navigate to the URL\n",
    "                driver.get(url)\n",
    "                \n",
    "                # Wait for JavaScript to load content (adjust time as needed)\n",
    "                time.sleep(5)\n",
    "                \n",
    "                # Get the page source after JavaScript has executed\n",
    "                html_content = driver.page_source\n",
    "                \n",
    "                # Close the browser\n",
    "                driver.quit()\n",
    "                \n",
    "                return html_content\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error with Selenium browser: {e}\")\n",
    "                return None\n",
    "        else:\n",
    "            # Keep using requests for other platforms like YouTube\n",
    "            try:\n",
    "                response = requests.get(url, headers=self.headers)\n",
    "                response.raise_for_status()\n",
    "                return response.text\n",
    "            except requests.RequestException as e:\n",
    "                print(f\"Error downloading content: {e}\")\n",
    "                return None\n",
    "    \n",
    "    def extract_youtube_data(self, html_content):\n",
    "        results = []\n",
    "        \n",
    "        # Extract initial data JSON\n",
    "        initial_data_match = re.search(r'var ytInitialData = (.+?);</script>', html_content)\n",
    "        if not initial_data_match:\n",
    "            print(\"Could not find YouTube initial data\")\n",
    "            return results\n",
    "            \n",
    "        try:\n",
    "            # Parse the video data from the extracted JSON\n",
    "            data_json = json.loads(initial_data_match.group(1))\n",
    "            contents = data_json.get('contents', {}).get('twoColumnSearchResultsRenderer', {}).get('primaryContents', {}).get('sectionListRenderer', {}).get('contents', [])\n",
    "            \n",
    "            for content in contents:\n",
    "                if 'itemSectionRenderer' in content:\n",
    "                    items = content.get('itemSectionRenderer', {}).get('contents', [])\n",
    "                    \n",
    "                    for item in items:\n",
    "                        if 'videoRenderer' in item:\n",
    "                            video_data = item['videoRenderer']\n",
    "                            \n",
    "                            # Extract video details using regex for safety\n",
    "                            video_id = video_data.get('videoId', '')\n",
    "                            \n",
    "                            # Title\n",
    "                            title_runs = video_data.get('title', {}).get('runs', [])\n",
    "                            title = ' '.join([run.get('text', '') for run in title_runs]) if title_runs else ''\n",
    "                            \n",
    "                            # Channel name\n",
    "                            channel_name = ''\n",
    "                            owner_text = video_data.get('ownerText', {}).get('runs', [])\n",
    "                            if owner_text:\n",
    "                                channel_name = owner_text[0].get('text', '')\n",
    "                            \n",
    "                            # View count\n",
    "                            view_count_text = video_data.get('viewCountText', {}).get('simpleText', '')\n",
    "                            view_count = re.search(r'(\\d[\\d,]*) views', view_count_text)\n",
    "                            view_count = view_count.group(1).replace(',', '') if view_count else '0'\n",
    "                            \n",
    "                            # Published time\n",
    "                            published_time = video_data.get('publishedTimeText', {}).get('simpleText', '')\n",
    "                            \n",
    "                            # Description\n",
    "                            description_snippet = video_data.get('detailedMetadataSnippets', [{}])\n",
    "                            description = ''\n",
    "                            if description_snippet:\n",
    "                                snippet_text = description_snippet[0].get('snippetText', {}).get('runs', [])\n",
    "                                description = ' '.join([run.get('text', '') for run in snippet_text])\n",
    "                            \n",
    "                            # URL\n",
    "                            url = f\"https://www.youtube.com/watch?v={video_id}\"\n",
    "                            \n",
    "                            video_info = {\n",
    "                                'platform': 'youtube',\n",
    "                                'title': title,\n",
    "                                'channel': channel_name,\n",
    "                                'description': description,\n",
    "                                'views': view_count,\n",
    "                                'published': published_time,\n",
    "                                'url': url,\n",
    "                                'video_id': video_id\n",
    "                            }\n",
    "                            \n",
    "                            results.append(video_info)\n",
    "                            \n",
    "                            if len(results) >= self.max_results:\n",
    "                                return results\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error parsing JSON data: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting YouTube data: {e}\")\n",
    "            \n",
    "        return results\n",
    "    \n",
    "    def extract_twitter_data(self, html_content):\n",
    "        results = []\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        \n",
    "        # Find tweets using regex patterns\n",
    "        tweet_pattern = re.compile(r'<article[^>]*data-testid=\"tweet\"[^>]*>(.*?)</article>', re.DOTALL)\n",
    "        tweets = tweet_pattern.findall(html_content)\n",
    "        \n",
    "        for tweet_html in tweets:\n",
    "            try:\n",
    "                # Create a BeautifulSoup object for the tweet\n",
    "                tweet_soup = BeautifulSoup(tweet_html, 'html.parser')\n",
    "                \n",
    "                # Extract author name\n",
    "                author_element = tweet_soup.select_one('[data-testid=\"User-Name\"]')\n",
    "                author = ''\n",
    "                if author_element:\n",
    "                    author_name_element = author_element.select_one('a span')\n",
    "                    if author_name_element:\n",
    "                        author = author_name_element.get_text().strip()\n",
    "                \n",
    "                # Extract username\n",
    "                username = ''\n",
    "                username_pattern = re.compile(r'@([A-Za-z0-9_]+)')\n",
    "                username_match = username_pattern.search(tweet_html)\n",
    "                if username_match:\n",
    "                    username = username_match.group(1)\n",
    "                \n",
    "                # Extract tweet text\n",
    "                text_element = tweet_soup.select_one('[data-testid=\"tweetText\"]')\n",
    "                text = text_element.get_text().strip() if text_element else ''\n",
    "                \n",
    "                # Extract time\n",
    "                time_element = tweet_soup.select_one('time')\n",
    "                timestamp = ''\n",
    "                if time_element:\n",
    "                    timestamp = time_element.get('datetime', '')\n",
    "                \n",
    "                # Extract tweet stats\n",
    "                reply_count = '0'\n",
    "                retweet_count = '0'\n",
    "                like_count = '0'\n",
    "                \n",
    "                reply_element = tweet_soup.select_one('[data-testid=\"reply\"]')\n",
    "                if reply_element:\n",
    "                    reply_text = reply_element.get_text()\n",
    "                    reply_match = re.search(r'(\\d+)', reply_text)\n",
    "                    if reply_match:\n",
    "                        reply_count = reply_match.group(1)\n",
    "                \n",
    "                retweet_element = tweet_soup.select_one('[data-testid=\"retweet\"]')\n",
    "                if retweet_element:\n",
    "                    retweet_text = retweet_element.get_text()\n",
    "                    retweet_match = re.search(r'(\\d+)', retweet_text)\n",
    "                    if retweet_match:\n",
    "                        retweet_count = retweet_match.group(1)\n",
    "                \n",
    "                like_element = tweet_soup.select_one('[data-testid=\"like\"]')\n",
    "                if like_element:\n",
    "                    like_text = like_element.get_text()\n",
    "                    like_match = re.search(r'(\\d+)', like_text)\n",
    "                    if like_match:\n",
    "                        like_count = like_match.group(1)\n",
    "                \n",
    "                # Extract tweet ID and URL\n",
    "                tweet_id = ''\n",
    "                tweet_url = ''\n",
    "                link_pattern = re.compile(r'https://twitter\\.com/[^/]+/status/(\\d+)')\n",
    "                link_match = link_pattern.search(tweet_html)\n",
    "                if link_match:\n",
    "                    tweet_id = link_match.group(1)\n",
    "                    tweet_url = f\"https://twitter.com/{username}/status/{tweet_id}\"\n",
    "                \n",
    "                tweet_info = {\n",
    "                    'platform': 'twitter',\n",
    "                    'author': author,\n",
    "                    'username': username,\n",
    "                    'text': text,\n",
    "                    'timestamp': timestamp,\n",
    "                    'replies': reply_count,\n",
    "                    'retweets': retweet_count,\n",
    "                    'likes': like_count,\n",
    "                    'tweet_id': tweet_id,\n",
    "                    'url': tweet_url\n",
    "                }\n",
    "                \n",
    "                results.append(tweet_info)\n",
    "                \n",
    "                if len(results) >= self.max_results:\n",
    "                    break\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error extracting tweet data: {e}\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def extract_data(self, html_content):\n",
    "        if self.platform == \"youtube\":\n",
    "            return self.extract_youtube_data(html_content)\n",
    "        elif self.platform == \"twitter\":\n",
    "            return self.extract_twitter_data(html_content)\n",
    "        return []\n",
    "    \n",
    "    def save_results(self, results):\n",
    "        with open(os.path.join('scraped_data/', self.output_file), 'w', encoding='utf-8') as f:\n",
    "            json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"Results saved to {self.output_file}\")\n",
    "    \n",
    "    def run(self):\n",
    "        print(f\"Scraping {self.platform} for keyword: {self.keyword}\")\n",
    "        url = self.get_search_url()\n",
    "        \n",
    "        # Download content\n",
    "        html_content = self.download_content(url)\n",
    "        if not html_content:\n",
    "            print(\"Failed to download content\")\n",
    "            return []\n",
    "        \n",
    "        # Save raw HTML for debugging\n",
    "        with open(f\"scraped_data/raw_{self.platform}_{self.keyword.replace(' ', '_')}.html\", 'w', encoding='utf-8') as f:\n",
    "            f.write(html_content)\n",
    "        \n",
    "        # Extract data\n",
    "        results = self.extract_data(html_content)\n",
    "        print(f\"Extracted {len(results)} items from {self.platform}\")\n",
    "        \n",
    "        # Save results\n",
    "        if results:\n",
    "            self.save_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping youtube for keyword: Etherium\n",
      "Extracted 19 items from youtube\n",
      "Results saved to youtube_Etherium_0402_183102.json\n"
     ]
    }
   ],
   "source": [
    "KEYWORDS = 'Etherium'\n",
    "MAX_RESULTS = 200\n",
    "\n",
    "scraper = ScraperPipeline('Youtube', KEYWORDS, None, MAX_RESULTS)\n",
    "scraper.run()\n",
    "\n",
    "# scraper = ScraperPipeline('twitter', KEYWORDS, 'twitter_output.json', MAX_RESULTS)\n",
    "# scraper.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
