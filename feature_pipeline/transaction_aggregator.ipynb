{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a92f61a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "badfc059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ./data_truth/data/.DS_Store does not match the expected pattern.\n",
      "\n",
      "Validators:\n",
      "./data_truth/data/eth_features_20210101_0000_20210201_0000\n",
      "type,chainId,nonce,gasPrice,gas,to,value,input,r,s,v,hash,blockHash,blockNumber,transactionIndex,from,blockTimestamp,datetime,valueETH\n",
      "./data_truth/data/eth_features_20210201_0000_20210228_2359\n",
      "type,chainId,nonce,gasPrice,gas,to,value,input,r,s,v,hash,blockHash,blockNumber,transactionIndex,from,blockTimestamp,datetime,valueETH\n",
      "./data_truth/data/eth_features_20210601_0000_20221001_0000\n",
      "blockHash,blockNumber,from,gas,gasPrice,hash,input,nonce,to,transactionIndex,value,type,chainId,v,r,s,blockTimestamp,datetime,valueETH\n",
      "./data_truth/data/eth_features_20220701_0000_20221001_0000\n",
      "type,chainId,nonce,gas,maxFeePerGas,maxPriorityFeePerGas,to,value,accessList,input,r,s,yParity,v,hash,blockHash,blockNumber,transactionIndex,from,gasPrice,blockTimestamp,datetime,valueETH\n",
      "./data_truth/data/eth_features_20221001_0000_20230101_0000\n",
      "blockHash,blockNumber,from,gas,gasPrice,maxPriorityFeePerGas,maxFeePerGas,hash,input,nonce,to,transactionIndex,value,type,accessList,chainId,v,yParity,r,s,blockTimestamp,datetime,valueETH,gasUsed,status\n",
      "./data_truth/data/eth_features_20210301_0000_20210310_0000\n",
      "blockHash,blockNumber,from,gas,gasPrice,hash,input,nonce,to,transactionIndex,value,type,chainId,v,r,s,blockTimestamp,datetime,valueETH,gasUsed,status\n",
      "./data_truth/data/eth_features_20210310_0000_20210401_0000\n",
      "blockHash,blockNumber,from,gas,gasPrice,hash,input,nonce,to,transactionIndex,value,type,chainId,v,r,s,blockTimestamp,datetime,valueETH\n",
      "./data_truth/data/eth_features_20210401_0000_20210501_0000\n",
      "type,chainId,nonce,gasPrice,gas,to,value,input,r,s,v,hash,blockHash,blockNumber,transactionIndex,from,blockTimestamp,datetime,valueETH\n",
      "./data_truth/data/eth_features_20220401_0000_20220701_0000\n",
      "type,chainId,nonce,gas,maxFeePerGas,maxPriorityFeePerGas,to,value,accessList,input,r,s,yParity,v,hash,blockHash,blockNumber,transactionIndex,from,gasPrice,blockTimestamp,datetime,valueETH,gasUsed,status\n",
      "./data_truth/data/eth_features_20210501_0000_20210601_0000\n",
      "type,chainId,nonce,gasPrice,gas,to,value,input,r,s,v,hash,blockHash,blockNumber,transactionIndex,from,blockTimestamp,datetime,valueETH\n",
      "./data_truth/data/eth_features_20211001_0000_20220101_0000\n",
      "type,chainId,nonce,gas,maxFeePerGas,maxPriorityFeePerGas,to,value,accessList,input,r,s,yParity,v,hash,blockHash,blockNumber,transactionIndex,from,gasPrice,blockTimestamp,datetime,valueETH\n",
      "\n",
      "Whales:\n",
      "./data_truth/data/eth_features_20210401_0000_20210501_0000\n",
      "blockHash,blockNumber,from,gas,gasPrice,hash,input,nonce,to,transactionIndex,value,type,chainId,v,r,s,blockTimestamp,datetime,valueETH,gasUsed,status\n",
      "./data_truth/data/eth_features_20210310_0000_20210401_0000\n",
      "type,chainId,nonce,gasPrice,gas,to,value,input,r,s,v,hash,blockHash,blockNumber,transactionIndex,from,blockTimestamp,datetime,valueETH,gasUsed,status\n",
      "./data_truth/data/eth_features_20210301_0000_20210310_0000\n",
      "blockHash,blockNumber,from,gas,gasPrice,hash,input,nonce,to,transactionIndex,value,type,chainId,v,r,s,blockTimestamp,datetime,valueETH,gasUsed,status\n",
      "./data_truth/data/eth_features_20210201_0000_20210228_2359\n",
      "type,chainId,nonce,gasPrice,gas,to,value,input,r,s,v,hash,blockHash,blockNumber,transactionIndex,from,blockTimestamp,datetime,valueETH,gasUsed,status\n",
      "./data_truth/data/eth_features_20210101_0000_20210201_0000\n",
      "type,chainId,nonce,gasPrice,gas,to,value,input,r,s,v,hash,blockHash,blockNumber,transactionIndex,from,blockTimestamp,datetime,valueETH,gasUsed,status\n",
      "./data_truth/data/eth_features_20210601_0000_20221001_0000\n",
      "type,chainId,nonce,gasPrice,gas,to,value,input,r,s,v,hash,blockHash,blockNumber,transactionIndex,from,blockTimestamp,datetime,valueETH,gasUsed,status\n",
      "./data_truth/data/eth_features_20220401_0000_20220701_0000\n",
      "blockHash,blockNumber,from,gas,gasPrice,maxPriorityFeePerGas,maxFeePerGas,hash,input,nonce,to,transactionIndex,value,type,accessList,chainId,v,yParity,r,s,blockTimestamp,datetime,valueETH,gasUsed,status\n",
      "./data_truth/data/eth_features_20220701_0000_20221001_0000\n",
      "type,chainId,nonce,gas,maxFeePerGas,maxPriorityFeePerGas,to,value,accessList,input,r,s,yParity,v,hash,blockHash,blockNumber,transactionIndex,from,gasPrice,blockTimestamp,datetime,valueETH,gasUsed,status\n",
      "./data_truth/data/eth_features_20211001_0000_20220101_0000\n",
      "blockHash,blockNumber,from,gas,gasPrice,hash,input,nonce,to,transactionIndex,value,type,chainId,v,r,s,blockTimestamp,datetime,valueETH,gasUsed,status\n",
      "./data_truth/data/eth_features_20210501_0000_20210601_0000\n",
      "type,chainId,nonce,gasPrice,gas,to,value,input,r,s,v,hash,blockHash,blockNumber,transactionIndex,from,blockTimestamp,datetime,valueETH,gasUsed,status\n",
      "./data_truth/data/eth_features_20221001_0000_20230101_0000\n",
      "blockHash,blockNumber,from,gas,gasPrice,maxPriorityFeePerGas,maxFeePerGas,hash,input,nonce,to,transactionIndex,value,type,accessList,chainId,v,yParity,r,s,blockTimestamp,datetime,valueETH,gasUsed,status\n"
     ]
    }
   ],
   "source": [
    "directory_path = './data_truth/data'\n",
    "all_files = [os.path.join(directory_path, f) for f in os.listdir(directory_path) if os.path.isfile(os.path.join(directory_path, f))]\n",
    "\n",
    "all_files_v = []\n",
    "all_files_t = []\n",
    "\n",
    "for file in all_files:\n",
    "    if file.endswith('_validator_transactions.csv'):\n",
    "        all_files_v.append(file)\n",
    "    elif file.endswith('_transactions.csv'):\n",
    "        all_files_t.append(file)\n",
    "    else:\n",
    "        print(f\"File {file} does not match the expected pattern.\")\n",
    "        \n",
    "print(\"\\nValidators:\")\n",
    "for file in all_files_v:\n",
    "    with open (file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        print(file.replace('_validator_transactions.csv', '').replace('./data/eth_features_', ''))\n",
    "        print(lines[0].strip('\\n'))\n",
    "\n",
    "print(\"\\nWhales:\")\n",
    "for file in all_files_t:\n",
    "    with open (file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        print(file.replace('_transactions.csv', '').replace('./data/eth_features_', ''))\n",
    "        print(lines[0].strip('\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73e99df3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAIL: ./data_truth/data/eth_features_20210101_0000_20210201_0000_validator_transactions.csv + Error tokenizing data. C error: Expected 19 fields in line 377, saw 21\n",
      "\n",
      "FAIL: ./data_truth/data/eth_features_20210201_0000_20210228_2359_validator_transactions.csv + Error tokenizing data. C error: Expected 19 fields in line 227, saw 21\n",
      "\n",
      "FAIL: ./data_truth/data/eth_features_20210601_0000_20221001_0000_validator_transactions.csv + Error tokenizing data. C error: Expected 19 fields in line 207, saw 21\n",
      "\n",
      "FAIL: ./data_truth/data/eth_features_20220701_0000_20221001_0000_validator_transactions.csv + Error tokenizing data. C error: Expected 23 fields in line 171, saw 25\n",
      "\n",
      "SUCESS: ./data_truth/data/eth_features_20221001_0000_20230101_0000_validator_transactions.csv\n",
      "SUCESS: ./data_truth/data/eth_features_20210301_0000_20210310_0000_validator_transactions.csv\n",
      "FAIL: ./data_truth/data/eth_features_20210310_0000_20210401_0000_validator_transactions.csv + Error tokenizing data. C error: Expected 19 fields in line 455, saw 21\n",
      "\n",
      "FAIL: ./data_truth/data/eth_features_20210401_0000_20210501_0000_validator_transactions.csv + Error tokenizing data. C error: Expected 19 fields in line 156, saw 21\n",
      "\n",
      "SUCESS: ./data_truth/data/eth_features_20220401_0000_20220701_0000_validator_transactions.csv\n",
      "FAIL: ./data_truth/data/eth_features_20210501_0000_20210601_0000_validator_transactions.csv + Error tokenizing data. C error: Expected 19 fields in line 887, saw 21\n",
      "\n",
      "FAIL: ./data_truth/data/eth_features_20211001_0000_20220101_0000_validator_transactions.csv + Error tokenizing data. C error: Expected 23 fields in line 243, saw 25\n",
      "\n",
      "SUCESS: ./data_truth/data/eth_features_20210401_0000_20210501_0000_transactions.csv\n",
      "SUCESS: ./data_truth/data/eth_features_20210310_0000_20210401_0000_transactions.csv\n",
      "SUCESS: ./data_truth/data/eth_features_20210301_0000_20210310_0000_transactions.csv\n",
      "SUCESS: ./data_truth/data/eth_features_20210201_0000_20210228_2359_transactions.csv\n",
      "SUCESS: ./data_truth/data/eth_features_20210101_0000_20210201_0000_transactions.csv\n",
      "FAIL: ./data_truth/data/eth_features_20210601_0000_20221001_0000_transactions.csv + Error tokenizing data. C error: Expected 21 fields in line 6560, saw 25\n",
      "\n",
      "SUCESS: ./data_truth/data/eth_features_20220401_0000_20220701_0000_transactions.csv\n",
      "SUCESS: ./data_truth/data/eth_features_20220701_0000_20221001_0000_transactions.csv\n",
      "FAIL: ./data_truth/data/eth_features_20211001_0000_20220101_0000_transactions.csv + Error tokenizing data. C error: Expected 21 fields in line 5, saw 25\n",
      "\n",
      "SUCESS: ./data_truth/data/eth_features_20210501_0000_20210601_0000_transactions.csv\n",
      "SUCESS: ./data_truth/data/eth_features_20221001_0000_20230101_0000_transactions.csv\n",
      "\n",
      "\n",
      "\n",
      "./data_truth/data/eth_features_20210101_0000_20210201_0000_validator_transactions.csv\n",
      "./data_truth/data/eth_features_20210201_0000_20210228_2359_validator_transactions.csv\n",
      "./data_truth/data/eth_features_20210601_0000_20221001_0000_validator_transactions.csv\n",
      "./data_truth/data/eth_features_20220701_0000_20221001_0000_validator_transactions.csv\n",
      "./data_truth/data/eth_features_20210310_0000_20210401_0000_validator_transactions.csv\n",
      "./data_truth/data/eth_features_20210401_0000_20210501_0000_validator_transactions.csv\n",
      "./data_truth/data/eth_features_20210501_0000_20210601_0000_validator_transactions.csv\n",
      "./data_truth/data/eth_features_20211001_0000_20220101_0000_validator_transactions.csv\n",
      "./data_truth/data/eth_features_20210601_0000_20221001_0000_transactions.csv\n",
      "./data_truth/data/eth_features_20211001_0000_20220101_0000_transactions.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "fail_list = []\n",
    "\n",
    "for file in all_files_v + all_files_t:\n",
    "    try:\n",
    "        _df = pd.read_csv(file)\n",
    "        print(f\"SUCESS: {file}\")\n",
    "    except Exception as e:\n",
    "        fail_list.append(file)\n",
    "        print(f\"FAIL: {file} + {e}\")\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "print('\\n'.join(fail_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78a7acda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: ./data/eth_features_20210301_0000_20210501_0000_transactions.csv\n",
      "Number of headers: 19\n",
      "Processing file: ./data/eth_features_20220501_0000_20220701_0000_transactions.csv\n",
      "Number of headers: 19\n",
      "Processing file: ./data/eth_features_20220101_0000_20220301_0000_validator_transactions.csv\n",
      "Number of headers: 19\n",
      "Processing file: ./data/eth_features_20220901_0000_20221101_0000_transactions.csv\n",
      "Number of headers: 19\n",
      "Processing file: ./data/eth_features_20210101_0000_20210301_0000_validator_transactions.csv\n",
      "Number of headers: 19\n",
      "Processing file: ./data/eth_features_20221101_0000_20230101_0000_validator_transactions.csv\n",
      "Number of headers: 19\n",
      "Processing file: ./data/eth_features_20210701_0000_20210901_0000_transactions.csv\n",
      "Number of headers: 19\n",
      "Processing file: ./data/eth_features_20211101_0000_20220101_0000_transactions.csv\n",
      "Number of headers: 19\n",
      "Processing file: ./data/eth_features_20210901_0000_20211101_0000_validator_transactions.csv\n",
      "Number of headers: 19\n",
      "Processing file: ./data/eth_features_20220301_0000_20220501_0000_transactions.csv\n",
      "Number of headers: 19\n",
      "Processing file: ./data/eth_features_20220901_0000_20221101_0000_validator_transactions.csv\n",
      "Number of headers: 19\n",
      "Processing file: ./data/eth_features_20220501_0000_20220701_0000_validator_transactions.csv\n",
      "Number of headers: 19\n",
      "Processing file: ./data/eth_features_20210501_0000_20210701_0000_validator_transactions.csv\n",
      "Number of headers: 19\n",
      "Processing file: ./data/eth_features_20210701_0000_20210901_0000_validator_transactions.csv\n",
      "Number of headers: 19\n",
      "Processing file: ./data/eth_features_20221101_0000_20230101_0000_transactions.csv\n",
      "Number of headers: 19\n",
      "Processing file: ./data/eth_features_20210501_0000_20210701_0000_transactions.csv\n",
      "Number of headers: 19\n",
      "Processing file: ./data/eth_features_20210901_0000_20211101_0000_transactions.csv\n",
      "Number of headers: 19\n",
      "Processing file: ./data/eth_features_20211101_0000_20220101_0000_validator_transactions.csv\n",
      "Number of headers: 19\n",
      "Processing file: ./data/eth_features_20220701_0000_20220901_0000_transactions.csv\n",
      "Number of headers: 19\n",
      "Processing file: ./data/eth_features_20210301_0000_20210501_0000_validator_transactions.csv\n",
      "Number of headers: 19\n",
      "Processing file: ./data/eth_features_20210101_0000_20210301_0000_transactions.csv\n",
      "Number of headers: 19\n",
      "Processing file: ./data/eth_features_20220301_0000_20220501_0000_validator_transactions.csv\n",
      "Number of headers: 19\n"
     ]
    }
   ],
   "source": [
    "def clean_csv(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    header = lines[0].strip().split(',')\n",
    "    num_headers = len(header)\n",
    "    print(f\"Processing file: {file_path}\")\n",
    "    print(f\"Number of headers: {num_headers}\")\n",
    "\n",
    "    cleaned_lines = [lines[0]]  # Keep the header line\n",
    "    for i, line in enumerate(lines[1:], start=2):\n",
    "        columns = line.strip().split(',')\n",
    "        if len(columns) > num_headers:\n",
    "            print(f\"Line {i} has {len(columns)} columns, cutting to {num_headers} columns.\")\n",
    "            columns = columns[:num_headers]\n",
    "        cleaned_lines.append(','.join(columns) + '\\n')\n",
    "\n",
    "    with open(file_path, 'w') as file:\n",
    "        file.writelines(cleaned_lines)\n",
    "\n",
    "directory_path = './data'\n",
    "all_files = [os.path.join(directory_path, f) for f in os.listdir(directory_path) if os.path.isfile(os.path.join(directory_path, f))]\n",
    "\n",
    "for file in all_files:\n",
    "    if file.endswith('.csv'):\n",
    "        clean_csv(file)\n",
    "\n",
    "validator_files = [file for file in all_files if file.endswith('_validator_transactions.csv')]\n",
    "transaction_files = [file for file in all_files if file.endswith('_transactions.csv') and not file.endswith('_validator_transactions.csv')]\n",
    "\n",
    "def aggregate_files(file_list):\n",
    "    dataframes = [pd.read_csv(file) for file in file_list]\n",
    "    return pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "if validator_files:\n",
    "    validators_combined = aggregate_files(validator_files)\n",
    "    validators_combined.to_csv('./data/validators_aggregated.csv', index=False)\n",
    "\n",
    "if transaction_files:\n",
    "    transactions_combined = aggregate_files(transaction_files)\n",
    "    transactions_combined.to_csv('./data/transactions_aggregated.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
